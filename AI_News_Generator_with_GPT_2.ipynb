{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "CZnwXNEHCOt9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jbCKH46KAgON"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enable more detailed stack trace for debugging\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 tokenizer download\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Setting a custom token for padding\n",
        "# Dataset (example structure)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encodings = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encodings['input_ids'].flatten(),\n",
        "            'attention_mask': encodings['attention_mask'].flatten(),\n",
        "            'labels': encodings['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "# Examples of texts\n",
        "texts = [\"This is a test\", \"Another test sentence\"]\n",
        "dataset = CustomDataset(texts, tokenizer)\n",
        "train_loader = DataLoader(dataset, batch_size=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "kZA7nU_CBFaR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Moving the model to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Update embed size if special tokens are added\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Optimizer settings\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "wf-o5qvjBFj0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Function for training epoch\n",
        "def train_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Moving data to the GPU\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Foresight\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# A function to evaluate the model\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "05P60XKdBFq7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training the model\n",
        "def train_epoch(model, data_loader, optimizer):\n",
        "    model = model.train()  # We transfer the model to the learning mode\n",
        "    losses = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "4zSGCMj7BFwJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model learning process\n",
        "for epoch in range(3):  # You can increase the number of epochs for better results\n",
        "    train_loss = train_epoch(model, train_loader, optimizer)\n",
        "    print(f'Epoch {epoch + 1}, Loss: {train_loss}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9V9zEcQAjxD",
        "outputId": "adcf1bd8-688a-46ad-d814-257ff397fa51"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.059285163879395\n",
            "Epoch 2, Loss: 8.004773139953613\n",
            "Epoch 3, Loss: 6.78363561630249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Transferring the model to evaluation mode\n",
        "\n",
        "# Text prompt\n",
        "input_text = \"The future of AI is\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "attention_mask = torch.ones(input_ids.shape, device=device)  # Creating attention mask\n",
        "\n",
        "# Text generation\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=100,  # Increasing the length of the generated text\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,  # We use the exemplary method for generation\n",
        "    top_k=50,  # We consider only the 50 most likely tokens\n",
        "    top_p=0.95,  # We use top-p sampling (Nucleus Sampling)\n",
        "    temperature=0.7  # \"Temperature\" for diversity management\n",
        ")\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNDCXunoB0p0",
        "outputId": "d4254f3a-be16-4719-b44d-dac7504e64fb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of AI is not in the future, but in the future it will be.\n",
            "\n",
            "The future of AI is not in the future, but in the future it will be.\n",
            "\n",
            "The future of AI is not in the future, but in the future it will be.\n",
            "\n",
            "The future of AI is not in the future, but in the future it will be.\n",
            "\n",
            "The future of AI is not in the future, but in the future it will be.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\n",
        "    \"The future of AI is\",\n",
        "    \"Artificial intelligence will\",\n",
        "    \"In the next decade, AI\",\n",
        "]\n",
        "\n",
        "for input_text in input_texts:\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM3X5x4VLG3B",
        "outputId": "8ebe4c44-c2a7-4891-947e-b84eaab61750"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The future of AI is\n",
            "Generated: The future of AI is unknown.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The Future of AI is Alive.\n",
            "\n",
            "The\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Artificial intelligence will\n",
            "Generated: Artificial intelligence will become an integral part of our lives, but I believe that it is not the future and will not be the future. The future will be technology that will be able to tell us what is real, what is in our dreams, what is in our dreams, what we want to do and what we need to do, and what we need to learn.\n",
            "\n",
            "The future will be the creation of a world with a human population that is capable of doing anything and everything in a\n",
            "\n",
            "Input: In the next decade, AI\n",
            "Generated: In the next decade, AI will become a powerful tool to drive innovation, develop new technologies and improve our lives. In our view, the next generation of AI will be a technological revolution in the field of AI research.\n",
            "\n",
            "AI is the future of human-centered decision making. It will transform the way we think and act. It will transform the way we live, work and think. AI is the future of personal choice. It is the future of our lives.\n",
            "\n",
            "The future of\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Transferring the model to evaluation mode\n",
        "\n",
        "# Text prompts\n",
        "input_texts = [\n",
        "    \"The future of AI is\",\n",
        "    \"Artificial intelligence will\",\n",
        "    \"In the next decade, AI\",\n",
        "]\n",
        "\n",
        "# Text generation with various parameters\n",
        "for input_text in input_texts:\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,  # Increasing the length of the generated text\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_k=50,  # We consider the 50 most likely tokens\n",
        "        top_p=0.95,  # Використовуємо top-p sampling\n",
        "        temperature=0.7  # \"Temperature\" for diversity management\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR65NERNLeWE",
        "outputId": "8defec41-3a2f-4d4a-fff4-1f1bec93f08a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The future of AI is\n",
            "Generated: The future of AI is very bright. In our next post, we'll look at some of the potential applications of AI in the future.\n",
            "\n",
            "The future of AI is very bright. In our next post, we'll look at some of the potential applications of AI in the future. How will AI evolve in the coming years?\n",
            "\n",
            "A major shift in the world of AI will be the introduction of artificial intelligence (AI). This is a new technology that will enable AI to better understand and\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Artificial intelligence will\n",
            "Generated: Artificial intelligence will be used to create all sorts of ways to improve our lives and our lives will never be the same. We will never be able to solve any real world problem, because we will never be able to solve any real problem.\n",
            "\n",
            "So if you want to know what is going on right now, you're going to want to read about the technology.\n",
            "\n",
            "The technology is very different from the technology that we're currently using. We're using the technology to create the worlds\n",
            "\n",
            "Input: In the next decade, AI\n",
            "Generated: In the next decade, AI will be the most advanced and powerful tool in the computing world.\n",
            "\n",
            "\"It's going to be very important to get the world to embrace this,\" said Todashek. \"But we need to do it with a very high degree of confidence.\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Transferring the model to evaluation mode\n",
        "\n",
        "# Text prompts\n",
        "input_texts = [\n",
        "    \"The future of AI is\",\n",
        "    \"Artificial intelligence will\",\n",
        "    \"In the next decade, AI\",\n",
        "]\n",
        "\n",
        "# Text generation with various parameters\n",
        "for input_text in input_texts:\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=100,  # Increasing the length of the generated text\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_k=50,  # We consider the 50 most likely tokens\n",
        "        top_p=0.95,  # We use top-p sampling\n",
        "        temperature=0.7  # \"Temperature\" for diversity management\n",
        "    )\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gove98PaLy3_",
        "outputId": "66b29ba2-1c1e-4e85-d074-2e1dc70ed900"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The future of AI is\n",
            "Generated: The future of AI is uncertain. There is no guarantee that the technology will continue to evolve, but the potential of AI is still great. This is why I am confident that AI will continue to revolutionize the world.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Artificial intelligence will\n",
            "Generated: Artificial intelligence will probably improve in the next decade as the technology progresses. But it's going to take a lot more than just the advent of AI to push humanity to the edge.\n",
            "\n",
            "\"It's going to take more than just the advent of AI to push humanity to the edge,\" said Scott Fitch, a professor of computer science at the University of California, Berkeley. \"It's going to take more than just the advent of AI to push humanity to the edge.\"\n",
            "\n",
            "We\n",
            "\n",
            "Input: In the next decade, AI\n",
            "Generated: In the next decade, AI and other advanced technologies will continue to make their way to the computer, but we will also see that many people will be completely different from us. The next generation of AI will be more complicated than humans were ever before, and many of the concepts we have developed will be the same as those we have developed for humans.\n",
            "\n",
            "The most important thing is to be as open and tolerant as possible about the way you approach AI. There will be no fear of getting in\n",
            "\n"
          ]
        }
      ]
    }
  ]
}